{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19b3e29e-2885-4ff9-9398-914234b3c8f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T21:00:09.084116Z",
     "iopub.status.busy": "2025-08-27T21:00:09.083800Z",
     "iopub.status.idle": "2025-08-27T21:00:09.094434Z",
     "shell.execute_reply": "2025-08-27T21:00:09.092562Z",
     "shell.execute_reply.started": "2025-08-27T21:00:09.084095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utils.py loaded: v0.2.12\n",
      "config.py loaded: v0.1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import importlib\n",
    "import transformers\n",
    "import torch\n",
    "import pathlib\n",
    "import smart_open\n",
    "from IPython.display import display\n",
    "from sagemaker.huggingface.processing import HuggingFaceProcessor\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import FrameworkProcessor\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.session import get_execution_role\n",
    "\n",
    "# Adding ../01_modules or ./01_modules to the system path so that we can load modules from \n",
    "# there as well\n",
    "if '__file__' in globals():\n",
    "    script_dir = pathlib.Path(__file__).parent.resolve()\n",
    "else:\n",
    "    script_dir = pathlib.Path().absolute()\n",
    "modules_path_in_dev = os.path.abspath(os.path.join(script_dir, '..', '01_modules'))\n",
    "modules_path_in_prod = os.path.abspath(os.path.join(script_dir, '01_modules'))\n",
    "if os.path.exists(modules_path_in_dev):\n",
    "    sys.path.append(modules_path_in_dev)\n",
    "if os.path.exists(modules_path_in_prod):\n",
    "    sys.path.append(modules_path_in_prod)\n",
    "\n",
    "\n",
    "# # Jupyter only reads a local module the first time after \n",
    "# # kernel start. Re-running a cell with \n",
    "# # \"from mymodulename import *\" would not change\n",
    "# # anything, even if the imported module has since changed.\n",
    "# # As a workaround, we need to directly load the module, \n",
    "# # use importlib.reload to reload it and then import * \n",
    "import utils\n",
    "_ = importlib.reload(utils)\n",
    "import config\n",
    "_ = importlib.reload(config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "676dcb9f-bed5-4d1b-ab82-b73abc619b57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T23:47:44.929004Z",
     "iopub.status.busy": "2025-08-27T23:47:44.928711Z",
     "iopub.status.idle": "2025-08-27T23:47:51.165220Z",
     "shell.execute_reply": "2025-08-27T23:47:51.164470Z",
     "shell.execute_reply.started": "2025-08-27T23:47:44.928981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done: 000000.gz, 1/1\n",
      "done: 000001.gz, 2/3\n",
      "done: 000002.gz, 3/6\n",
      "done: 000003.gz, 1/7\n",
      "done: 000004.gz, 4/11\n",
      "done: 000005.gz, 2/13\n",
      "done: 000006.gz, 1/14\n",
      "done: 000007.gz, 1/15\n",
      "done: 000008.gz, 0/15\n",
      "done: 000009.gz, 0/15\n",
      "done: 000010.gz, 0/15\n",
      "done: 000011.gz, 0/15\n",
      "done: 000012.gz, 0/15\n",
      "done: 000013.gz, 0/15\n",
      "done: 000014.gz, 0/15\n",
      "done: 000015.gz, 0/15\n",
      "done: 000016.gz, 0/15\n",
      "done: 000017.gz, 0/15\n",
      "done: 000018.gz, 0/15\n",
      "done: 000019.gz, 0/15\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "openalex_works_source_prefix = f'{config.OPENALEX_S3_RAW_DATA_PREFIX}/data/works_unpartitioned/'\n",
    "openalex_works_source_path = f's3://{config.DEFAULT_S3_BUCKET_NAME}/{openalex_works_source_prefix}'\n",
    "openalex_works_target_prefix = f'{config.OPENALEX_S3_RAW_DATA_PREFIX}/data/works_reduced/'\n",
    "openalex_works_target_path = f's3://{config.DEFAULT_S3_BUCKET_NAME}/{openalex_works_target_prefix}'\n",
    "\n",
    "_ = \"\"\"\n",
    "for key, content in smart_open.s3.iter_bucket(\n",
    "    bucket_name=config.DEFAULT_S3_BUCKET_NAME,\n",
    "    prefix=openalex_works_source_prefix,\n",
    "    accept_key=None,\n",
    "    key_limit=16,\n",
    "    workers=16,\n",
    "    retries=3,\n",
    "):\n",
    "    print(key, round(len(content) / 2**10))\n",
    "\"\"\"\n",
    "\n",
    "file_counter = 0\n",
    "file_limit = 20\n",
    "line_limit = 10\n",
    "total_record_counter = 0\n",
    "total_record_limit = 15\n",
    "files = s3_client.list_objects(Bucket=config.DEFAULT_S3_BUCKET_NAME, Prefix='01_data/01_raw/openalex/data/works_unpartitioned/', Delimiter='/', MaxKeys=30)\n",
    "# print(files)\n",
    "# print('/n')\n",
    "\n",
    "def line_has_error(line:str)->bool:\n",
    "    known_keys = [\n",
    "        'id', 'doi', 'doi_registration_agency', 'display_name', 'title', \n",
    "        'publication_year', 'publication_date', 'language', 'ids', 'primary_location', \n",
    "        'best_oa_location', 'type', 'open_access', 'authorships', 'corresponding_author_ids',\n",
    "        'corresponding_institution_ids', 'cited_by_count', 'summary_stats', 'biblio',\n",
    "        'is_retracted', 'is_paratext', 'concepts', 'mesh', 'locations_count', 'locations',\n",
    "        'referenced_works', 'referenced_works_count', 'sustainable_development_goals',\n",
    "        'grants', 'apc_list', 'apc_paid', 'related_works', 'abstract_inverted_index',\n",
    "        'counts_by_year', 'cited_by_api_url', 'updated_date', 'created_date', 'updated',\n",
    "        'authors_count', 'concepts_count', 'type_crossref', 'institutions_distinct_count'\n",
    "    ]\n",
    "    if (\n",
    "\n",
    "        line.find('prospective')>-1 and\n",
    "        line.find('percutaneous')>-1 and\n",
    "        line.find('transluminal')>-1 and\n",
    "        line.find('angioplasty')>-1 and\n",
    "        line.find('prospective')>-1\n",
    "    ):\n",
    "        return 'Might be the problematic file'\n",
    "    try:\n",
    "        line_dict = json.loads(line)\n",
    "    except:\n",
    "        return 'Could not json.load() line'\n",
    "    else:\n",
    "        unknown_keys = list(set(line_dict.keys()) - set(known_keys))\n",
    "        if len(unknown_keys):\n",
    "            return f'Unknown keys: {unknown_keys}'\n",
    "    return False\n",
    "\n",
    "_ = \"\"\"\n",
    "for file_ref in files['Contents']:\n",
    "    if file_counter < file_limit:\n",
    "        line_counter = 0\n",
    "        source_filepath = file_ref['Key']\n",
    "        source_filename = source_filepath.split('/')[-1]\n",
    "        with smart_open.open(f's3://{config.DEFAULT_S3_BUCKET_NAME}/{source_filepath}') as fl:\n",
    "            for line in fl:\n",
    "                if line_counter >= line_limit or total_record_counter >= total_record_limit:\n",
    "                    break\n",
    "                line_error = line_has_error(line)\n",
    "                if line_error:\n",
    "                    print(f'{source_filename}:{line_counter:06}|{line_error}|{line}')\n",
    "                else:\n",
    "                    print(f'{source_filename}:{line_counter:06}|--')\n",
    "                line_counter += 1\n",
    "                total_record_counter += 1\n",
    "        file_counter += 1\n",
    "\"\"\"\n",
    "\n",
    "def reduce_line(source_filepath:str, line_counter:int, line:str)->str:\n",
    "    try:\n",
    "        line_dict = json.loads(line)\n",
    "    except:\n",
    "        print(f'ERROR: Could not parse JSON from [{source_filepath}]:{line_counter}: {line}\\n')\n",
    "        return ''\n",
    "    else:\n",
    "        reduced_line = {}\n",
    "        if 'id' in line_dict:\n",
    "            reduced_line['id_openalex_long'] = line_dict['id'] # https://openalex.org/W2079861989\n",
    "            try:\n",
    "                reduced_line['id_openalex_short'] = line_dict['id'].split('/')[-1][1:] # 2079861989\n",
    "            except:\n",
    "                print(f'Warning: could not split {key} in [{source_filepath}]:{line_counter}: {line}')\n",
    "        else:\n",
    "            print(f'ERROR: Line does not contain ID [{source_filepath}]:{line_counter}: {line}\\n')\n",
    "            return ''\n",
    "            \n",
    "        key = 'doi'\n",
    "        if key in line_dict:\n",
    "            reduced_line['id_doi_long'] = line_dict[key]\n",
    "            try:\n",
    "                reduced_line['id_doi_short'] = None if line_dict[key] is None else '/'.join(line_dict[key].split('/')[3:])\n",
    "            except:\n",
    "                print(f'Warning: could not split {key} in [{source_filepath}]:{line_counter}: {line}')\n",
    "            \n",
    "        key = 'display_name'\n",
    "        if key in line_dict:\n",
    "            reduced_line[key] = line_dict[key]\n",
    "            \n",
    "        key = 'title'\n",
    "        if key in line_dict:\n",
    "            reduced_line[key] = line_dict[key]\n",
    "            \n",
    "        key = 'language'\n",
    "        if key in line_dict:\n",
    "            reduced_line[key] = line_dict[key]\n",
    "            \n",
    "        key = 'publication_year'\n",
    "        if key in line_dict:\n",
    "            reduced_line[key] = line_dict[key]\n",
    "            \n",
    "        key = 'ids'\n",
    "        if key in line_dict:\n",
    "            subkey = 'pmid'\n",
    "            if subkey in line_dict[key]:\n",
    "                reduced_line[f'id_{subkey}_long'] = line_dict[key][subkey]\n",
    "                try:\n",
    "                    reduced_line[f'id_{subkey}_short'] = None if line_dict[key][subkey] is None else '/'.join(line_dict[key][subkey].split('/')[3:])\n",
    "                except:\n",
    "                    print(f'Warning: could not split {key}_{subkey} in [{source_filepath}]:{line_counter}: {line}')\n",
    "            subkey = 'mag'\n",
    "            if subkey in line_dict[key]:\n",
    "                reduced_line[f'id_{subkey}'] = line_dict[key][subkey]\n",
    "        \n",
    "        key = 'type'\n",
    "        if key in line_dict:\n",
    "            reduced_line['item_type'] = line_dict[key]\n",
    "\n",
    "        key = 'primary_topic'\n",
    "        if key in line_dict:\n",
    "            subkey = 'id'\n",
    "            if subkey in line_dict[key]:\n",
    "                reduced_line[f'{key}_long_id'] = line_dict[key][subkey] # https://openalex.org/T10062\n",
    "                try:\n",
    "                    reduced_line[f'{key}_short_id'] = None if line_dict[key][subkey] is None else line_dict[key][subkey].split('/')[-1] # T10062\n",
    "                except:\n",
    "                    print(f'Warning: could not split {key}_{subkey} in [{source_filepath}]:{line_counter}: {line}')\n",
    "            subkey = 'display_name'\n",
    "            if subkey in line_dict[key]:\n",
    "                reduced_line[f'{key}_{subkey}'] = line_dict[key][subkey]\n",
    "            subkey = 'score'\n",
    "            if subkey in line_dict[key]:\n",
    "                reduced_line[f'{key}_{subkey}'] = line_dict[key][subkey]\n",
    "            subkey = 'subfield'\n",
    "            if subkey in line_dict[key]:\n",
    "                subsubkey = 'id'\n",
    "                if subsubkey in line_dict[key][subkey]:\n",
    "                    reduced_line[f'{key}_{subkey}_long_id'] = line_dict[key][subkey][subsubkey] # https://openalex.org/subfields/1306\n",
    "                    try:\n",
    "                        reduced_line[f'{key}_{subkey}_short_id'] = None if line_dict[key][subkey][subsubkey] is None else line_dict[key][subkey][subsubkey].split('/')[-1] # 1306\n",
    "                    except:\n",
    "                        print(f'Warning: could not split {key}_{subkey}_{subsubkey} in [{source_filepath}]:{line_counter}: {line}')\n",
    "                subkey = 'display_name'\n",
    "                if subsubkey in line_dict[key][subkey]:\n",
    "                    reduced_line[f'{key}_{subkey}_{subsubkey}'] = line_dict[key][subkey][subsubkey]\n",
    "            subkey = 'field'\n",
    "            if subkey in line_dict[key]:\n",
    "                subsubkey = 'id'\n",
    "                if subsubkey in line_dict[key][subkey]:\n",
    "                    reduced_line[f'{key}_{subkey}_long_id'] = line_dict[key][subkey][subsubkey] # https://openalex.org/subfields/1306\n",
    "                    try:\n",
    "                        reduced_line[f'{key}_{subkey}_short_id'] = None if line_dict[key][subkey][subsubkey] is None else line_dict[key][subkey][subsubkey].split('/')[-1] # 1306\n",
    "                    except:\n",
    "                        print(f'Warning: could not split {key}_{subkey}_{subsubkey} in [{source_filepath}]:{line_counter}: {line}')\n",
    "                subkey = 'display_name'\n",
    "                if subsubkey in line_dict[key][subkey]:\n",
    "                    reduced_line[f'{key}_{subkey}_{subsubkey}'] = line_dict[key][subkey][subsubkey]\n",
    "            subkey = 'domain'\n",
    "            if subkey in line_dict[key]:\n",
    "                subsubkey = 'id'\n",
    "                if subsubkey in line_dict[key][subkey]:\n",
    "                    reduced_line[f'{key}_{subkey}_long_id'] = line_dict[key][subkey][subsubkey] # https://openalex.org/subfields/1306\n",
    "                    try:\n",
    "                        reduced_line[f'{key}_{subkey}_short_id'] = None if line_dict[key][subkey][subsubkey] is None else line_dict[key][subkey][subsubkey].split('/')[-1] # 1306\n",
    "                    except:\n",
    "                        print(f'Warning: could not split {key}_{subkey}_{subsubkey} in [{source_filepath}]:{line_counter}: {line}')\n",
    "                subkey = 'display_name'\n",
    "                if subsubkey in line_dict[key][subkey]:\n",
    "                    reduced_line[f'{key}_{subkey}_{subsubkey}'] = line_dict[key][subkey][subsubkey]\n",
    "    return json.dumps(reduced_line, default=str)\n",
    "\n",
    "for file_ref in files['Contents']:\n",
    "    if file_counter < file_limit:\n",
    "        line_counter = 0\n",
    "        source_filepath = file_ref['Key']\n",
    "        target_filepath = source_filepath.replace('works_unpartitioned', 'works_reduced')\n",
    "        source_filename = source_filepath.split('/')[-1]\n",
    "        target_file_location = f's3://{config.DEFAULT_S3_BUCKET_NAME}/{target_filepath}'\n",
    "        with smart_open.open(f's3://{config.DEFAULT_S3_BUCKET_NAME}/{source_filepath}') as file_source:\n",
    "            with smart_open.open(target_file_location, 'w') as file_target:\n",
    "                for line in file_source:\n",
    "                    if line_counter >= line_limit or total_record_counter >= total_record_limit:\n",
    "                        break\n",
    "                    reduced_line = reduce_line(source_filepath, line_counter, line)\n",
    "                    if reduced_line != '':\n",
    "                        file_target.write(reduced_line)\n",
    "    \n",
    "                    # print(f'{source_filename}:{line_counter:06}|')\n",
    "                    # print(reduced_line)\n",
    "                    # print('\\n\\n')\n",
    "                    line_counter += 1\n",
    "                    total_record_counter += 1\n",
    "        print(f'done: {source_filename}, {line_counter}/{total_record_counter}')\n",
    "        file_counter += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_311_2",
   "language": "python",
   "name": "python_311_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
