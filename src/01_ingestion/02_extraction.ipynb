{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc05eb-0f15-44bf-a0bd-181ff50269fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import gzip\n",
    "import os\n",
    "from botocore.exceptions import ClientError\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ProgressBar(tqdm):\n",
    "    \"\"\"Custom progress bar for S3 operations.\"\"\"\n",
    "\n",
    "    def update_to(self, bytes_transferred):\n",
    "        self.update(bytes_transferred - self.n)\n",
    "\n",
    "\n",
    "def download_file_with_progress(s3_client, bucket_name, key, local_file):\n",
    "    \"\"\"\n",
    "    Downloads a file from S3 with a progress bar.\n",
    "\n",
    "    Args:\n",
    "        s3_client: The boto3 S3 client.\n",
    "        bucket_name (str): The name of the S3 bucket.\n",
    "        key (str): The key of the file in the S3 bucket.\n",
    "        local_file (str): The local file path to save the downloaded file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Get the file size from S3\n",
    "    file_size = s3_client.head_object(Bucket=bucket_name, Key=key)['ContentLength']\n",
    "\n",
    "    # Use tqdm for the progress bar\n",
    "    with ProgressBar(total=file_size, unit=\"B\", unit_scale=True, desc=\"Downloading\") as progress_bar:\n",
    "        s3_client.download_file(\n",
    "            Bucket=bucket_name,\n",
    "            Key=key,\n",
    "            Filename=local_file,\n",
    "            Callback=progress_bar.update_to\n",
    "        )\n",
    "\n",
    "\n",
    "def upload_file_with_progress(s3_client, local_file, bucket_name, key):\n",
    "    \"\"\"\n",
    "    Uploads a file to S3 with a progress bar.\n",
    "\n",
    "    Args:\n",
    "        s3_client: The boto3 S3 client.\n",
    "        local_file (str): The local file path to upload.\n",
    "        bucket_name (str): The name of the S3 bucket.\n",
    "        key (str): The key for the file in the S3 bucket.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Get the file size\n",
    "    file_size = os.path.getsize(local_file)\n",
    "\n",
    "    # Use tqdm for the progress bar\n",
    "    with ProgressBar(total=file_size, unit=\"B\", unit_scale=True, desc=\"Uploading\") as progress_bar:\n",
    "        s3_client.upload_file(\n",
    "            Filename=local_file,\n",
    "            Bucket=bucket_name,\n",
    "            Key=key,\n",
    "            Callback=progress_bar.update_to\n",
    "        )\n",
    "\n",
    "        \n",
    "def download_extract_upload_s3(bucket_name, gz_key, extracted_key, aws_region=\"us-east-1\"):\n",
    "    \"\"\"\n",
    "    Downloads a .gz file from S3, extracts its contents, and uploads the extracted file back to S3.\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): The name of the S3 bucket.\n",
    "        gz_key (str): The key of the .gz file in the S3 bucket.\n",
    "        extracted_key (str): The key for the extracted file to be uploaded back to S3.\n",
    "        aws_region (str): The AWS region of the S3 bucket.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client(\"s3\", region_name=aws_region)\n",
    "    local_extracted_file = extracted_key.split('/')[-1]\n",
    "    local_gz_file = f'{local_extracted_file}.gz'\n",
    "    \n",
    "\n",
    "    try:\n",
    "        # Step 1: Download the .gz file from S3\n",
    "        print(f\"Downloading {gz_key} from S3 bucket {bucket_name}...\")\n",
    "        download_file_with_progress(s3_client, bucket_name, gz_key, local_gz_file)\n",
    "        print(f\"Downloaded {gz_key} to {local_gz_file}\")\n",
    "\n",
    "        with gzip.open(local_gz_file, 'rt') as gz_file:\n",
    "            # Get the total size of the compressed file\n",
    "            gz_file_size = os.path.getsize(local_gz_file)\n",
    "            with tqdm(total=gz_file_size, unit='B', unit_scale=True, desc=f'Extracting {local_gz_file}') as progress_bar:\n",
    "                with open(local_extracted_file, 'W') as extracted_file:\n",
    "                    while chunk := gz_file.read(8192):  # Read in chunks\n",
    "                        extracted_file.write(chunk)\n",
    "                        progress_bar.update(len(chunk))\n",
    "        print(f'Extracted content saved to {local_extracted_file}')\n",
    "\n",
    "        # Step 3: Upload the extracted file back to S3\n",
    "        print(f\"Uploading extracted file to S3 bucket {bucket_name} with key {extracted_key}...\")\n",
    "        upload_file_with_progress(s3_client, local_extracted_file, bucket_name, extracted_key)\n",
    "        print(f\"Uploaded extracted file to s3://{bucket_name}/{extracted_key}\")\n",
    "\n",
    "    except ClientError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        # Clean up local temporary files\n",
    "        if os.path.exists(local_gz_file):\n",
    "            os.remove(local_gz_file)\n",
    "        if os.path.exists(local_extracted_file):\n",
    "            os.remove(local_extracted_file)\n",
    "\n",
    "\n",
    "def process_all_files(bucket_name, prefix, aws_region=\"us-east-1\", min_index=0, max_index=100000):\n",
    "    \"\"\"\n",
    "    Processes all .gz files in an S3 bucket with a given prefix.\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): The name of the S3 bucket.\n",
    "        prefix (str): The prefix of the .gz files in the S3 bucket.\n",
    "        aws_region (str): The AWS region of the S3 bucket.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client(\"s3\", region_name=aws_region)\n",
    "\n",
    "    try:\n",
    "        # List all .gz files in the bucket with the given prefix\n",
    "        print(f\"Listing .gz files in bucket {bucket_name} with prefix {prefix}...\")\n",
    "        response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "        if \"Contents\" not in response:\n",
    "            print(\"No files found.\")\n",
    "            return\n",
    "\n",
    "        counter = 0\n",
    "        for obj in response[\"Contents\"]:\n",
    "            if counter >= min_index and counter < max_index:\n",
    "                gz_key = obj[\"Key\"]\n",
    "                if gz_key.endswith(\".gz\"):\n",
    "                    # Define the key for the extracted file\n",
    "                    extracted_key = gz_key.replace(\".gz\", \"\")\n",
    "                    print(f\"Processing file: {gz_key}\")\n",
    "                    download_extract_upload_s3(bucket_name, gz_key, extracted_key, aws_region)\n",
    "            counter += 1\n",
    "\n",
    "    except ClientError as e:\n",
    "        print(f\"Error listing files in S3: {e}\")\n",
    "\n",
    "\n",
    "bucket_name = \"steve-sagemaker-data-bucket\"\n",
    "prefix = \"papers/\"  # Prefix for the .gz files\n",
    "aws_region = \"eu-west-2\"  # AWS region of the bucket\n",
    "\n",
    "process_all_files(bucket_name, prefix, aws_region, 0, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
