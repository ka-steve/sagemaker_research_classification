{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc05eb-0f15-44bf-a0bd-181ff50269fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import gzip\n",
    "import os\n",
    "import awswrangler as wr\n",
    "from tqdm import tqdm\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "class ProgressBar(tqdm):\n",
    "    \"\"\"Custom progress bar for S3 operations.\"\"\"\n",
    "\n",
    "    def update_to(self, bytes_transferred):\n",
    "        self.update(bytes_transferred - self.n)\n",
    "\n",
    "\n",
    "def download_file_with_progress(s3_client, bucket_name, key, local_file):\n",
    "    \"\"\"\n",
    "    Downloads a file from S3 with a progress bar.\n",
    "\n",
    "    Args:\n",
    "        s3_client: The boto3 S3 client.\n",
    "        bucket_name (str): The name of the S3 bucket.\n",
    "        key (str): The key of the file in the S3 bucket.\n",
    "        local_file (str): The local file path to save the downloaded file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    file_size = s3_client.head_object(Bucket=bucket_name, Key=key)['ContentLength']\n",
    "    with ProgressBar(total=file_size, unit=\"B\", unit_scale=True, desc=\"Downloading\") as progress_bar:\n",
    "        s3_client.download_file(\n",
    "            Bucket=bucket_name,\n",
    "            Key=key,\n",
    "            Filename=local_file,\n",
    "            Callback=progress_bar.update_to\n",
    "        )\n",
    "\n",
    "\n",
    "def download_extract_save_to_athena(counter, bucket_name, gz_key, s3_output_prefix, glue_database, glue_table, aws_region=\"us-east-1\"):\n",
    "    \"\"\"\n",
    "    Downloads a .gz file from S3, extracts its contents, and saves it to S3 as a dataset using awswrangler.\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): The name of the S3 bucket.\n",
    "        gz_key (str): The key of the .gz file in the S3 bucket.\n",
    "        s3_output_prefix (str): The S3 prefix where the dataset will be saved.\n",
    "        glue_database (str): The Glue database name.\n",
    "        glue_table (str): The Glue table name.\n",
    "        aws_region (str): The AWS region of the S3 bucket.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client(\"s3\", region_name=aws_region)\n",
    "    extracted_key = gz_key.replace(\".gz\", \"\")\n",
    "    local_extracted_file = extracted_key.split('/')[-1]\n",
    "    local_gz_file = f'{local_extracted_file}.gz'\n",
    "\n",
    "    try:\n",
    "        # Step 1: Download the .gz file from S3\n",
    "        print(f\"Downloading {gz_key} from S3 bucket {bucket_name}...\")\n",
    "        download_file_with_progress(s3_client, bucket_name, gz_key, local_gz_file)\n",
    "        print(f\"Downloaded {gz_key} to {local_gz_file}\")\n",
    "\n",
    "        # Step 2: Extract the .gz file\n",
    "        print(f\"Extracting {local_gz_file}...\")\n",
    "        with gzip.open(local_gz_file, 'rt') as gz_file:\n",
    "            gz_file_size = os.path.getsize(local_gz_file)\n",
    "            with tqdm(total=gz_file_size, unit='B', unit_scale=True, desc=f'Extracting {local_gz_file}') as progress_bar:\n",
    "                with open(local_extracted_file, 'w') as extracted_file:\n",
    "                    while chunk := gz_file.read(8192):\n",
    "                        extracted_file.write(chunk)\n",
    "                        progress_bar.update(len(chunk))\n",
    "        print(f'Extracted content saved to {local_extracted_file}')\n",
    "\n",
    "        # Step 3: Save the extracted file to S3 as a dataset and register it in Athena\n",
    "        print(f\"Saving {local_extracted_file} to S3 as a dataset and registering it in Athena...\")\n",
    "        s3_output_path = f\"s3://{bucket_name}/{s3_output_prefix}\"\n",
    "    \n",
    "        # Read the JSONL file into a Pandas DataFrame\n",
    "        df = wr.s3.read_json(local_extracted_file, lines=True)\n",
    "        mode = 'append'\n",
    "        if counter == 0:\n",
    "            mode = 'overwrite'\n",
    "    \n",
    "        # Save the DataFrame to S3 as a dataset\n",
    "        wr.s3.to_json(\n",
    "            df=df,\n",
    "            path=s3_output_path,\n",
    "            dataset=True,\n",
    "            database=glue_database,\n",
    "            table=glue_table,\n",
    "            mode=mode\n",
    "        )\n",
    "        print(f\"Dataset saved to {s3_output_path} and registered in Athena (Glue database: {glue_database}, table: {glue_table}).\")\n",
    "\n",
    "    except ClientError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "#     finally:\n",
    "#         # Clean up local temporary files\n",
    "#         if os.path.exists(local_gz_file):\n",
    "#             os.remove(local_gz_file)\n",
    "#         if os.path.exists(local_extracted_file):\n",
    "#             os.remove(local_extracted_file)\n",
    "\n",
    "\n",
    "def extract_all_files(bucket_name, prefix, s3_output_prefix, glue_database, glue_table, aws_region=\"us-east-1\", min_index=0, max_index=100000):\n",
    "    \"\"\"\n",
    "    Processes all .gz files in an S3 bucket with a given prefix.\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): The name of the S3 bucket.\n",
    "        prefix (str): The prefix of the .gz files in the S3 bucket.\n",
    "        s3_output_prefix (str): The S3 prefix where the dataset will be saved.\n",
    "        glue_database (str): The Glue database name.\n",
    "        glue_table (str): The Glue table name.\n",
    "        aws_region (str): The AWS region of the S3 bucket.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client(\"s3\", region_name=aws_region)\n",
    "\n",
    "    try:\n",
    "        # List all .gz files in the bucket with the given prefix\n",
    "        print(f\"Listing .gz files in bucket {bucket_name} with prefix {prefix}...\")\n",
    "        response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "        if \"Contents\" not in response:\n",
    "            print(\"No files found.\")\n",
    "            return\n",
    "\n",
    "        counter = 0\n",
    "        for obj in response[\"Contents\"]:\n",
    "            if counter >= min_index and counter < max_index:\n",
    "                gz_key = obj[\"Key\"]\n",
    "                if gz_key.endswith(\".gz\"):\n",
    "                    print(f\"Processing file: {gz_key}\")\n",
    "                    download_extract_save_to_athena(counter, bucket_name, gz_key, s3_output_prefix, glue_database, glue_table, aws_region)\n",
    "            counter += 1\n",
    "\n",
    "    except ClientError as e:\n",
    "        print(f\"Error listing files in S3: {e}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "bucket_name = \"steve-sagemaker-data-bucket\"\n",
    "prefix = \"00_raw/papers/\"  # Prefix for the .gz files\n",
    "s3_output_prefix = \"01_extracted/papers/\"  # S3 prefix for the output dataset\n",
    "glue_database = \"s2\"  # Glue database name\n",
    "glue_table = \"papers\"  # Glue table name\n",
    "aws_region = \"eu-west-2\"  # AWS region of the bucket\n",
    "\n",
    "extract_all_files(bucket_name, prefix, s3_output_prefix, glue_database, glue_table, aws_region, 0, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
